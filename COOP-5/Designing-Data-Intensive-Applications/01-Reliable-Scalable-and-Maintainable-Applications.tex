% Created 2025-01-12 Sun 16:21
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{parskip,darkmode}
\enabledarkmode
\author{Arnav Gupta}
\date{\today}
\title{Reliable, Scalable, and Maintainable Applications}
\hypersetup{
 pdfauthor={Arnav Gupta},
 pdftitle={Reliable, Scalable, and Maintainable Applications},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.7.19)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Introduction}
\label{sec:orgbfcfe6e}
\textbf{Data-intensive applications} are built from standard building blocks that provide commonly needed
functionality.
These building blocks include \uline{databases}, \uline{caches}, \uline{search indexes}, \uline{stream processing}, and
\uline{batch processing}.

When building an application, its important to figure out which tools and approaches are most appropriate
for the task at hand.
\section{Thinking About Data Systems}
\label{sec:org56a4134}
In recent years, boundaries between categories of data systems are becoming blurred.

Work is broken down into tasks that can be performed efficiently on a single tool, with different tools
stitched together using application code.

When combining tools to provide a service, the API hides implementation details from clients.
\section{Reliability}
\label{sec:org6d3175d}
\textbf{Reliability}: the system should continue to work \emph{correctly} (perform correct function at desired level
of performance) even in the face of \emph{adversity} (hardware or software faults, even human error)

More roughly, reliability means to continue to work correctly, even when things go wrong.

\textbf{Fault}: a component of the system deviating from the spec

\textbf{Fault-tolerant/resilient}: systems that anticipate faults and can cope with them

\textbf{Failure}: when the system as a whole stops providing the required service to the user

Impossible to reduce probability of a fault to zero, so best to prevent faults from causing failures.
Generally better to tolerate faults than prevent faults, but must also sometimes prevent faults.

One strategy involves deliberately inducing faults.
\subsection{Hardware Faults}
\label{sec:org745665b}
Hard disks have a mean time to failure (MTTF), so with many disks, some number of disks will die every day.

Can add redundancy to individual hardware components to reduce the failure rate of the system (like using
RAID, backup power, etc).
This cannot completely prevent hardware problems from causing failures, but is well understood and has
decent success rate.

Redundancy was sufficient in the past since if a backup is restored onto a new machine fairly quickly,
downtime in case of failure is not catastrophic in most applications.

As data volumes and computing demands increased, more applications use more machines and so hardware
fault rates have increased.
Platforms are now designed to prioritize flexibility and elasticity over single-machine reliability.

Systems should tolerate the loss of entire machines using software and hardware techniques.
\subsection{Software Faults}
\label{sec:org8024529}
Hardware faults are generally have no correlation.

Systematic errors within the system are correlated across nodes and cause more system failures.
This includes software bugs, runaway processes, slowdowns, and cascading failures.

Software faults often lie dormant and are triggered by an unusual set of circumstances.

Solutions include:
\begin{itemize}
\item considering assumptions made
\item thorough testing
\item process isolation
\item monitoring system behaviour
\end{itemize}
\subsection{Human Errors}
\label{sec:org28452e2}
Humans can be unreliable. To mitigate this:
\begin{itemize}
\item design system to minimize opportunities for error
\item decouple places where people make mistakes from places where they can cause failures
\item test thoroughly at all levels
\item allow quick and easy recovery from human errors
\item setup detailed and clear monitoring, called \textbf{telemetry}
\item implement good management practices and training
\end{itemize}
\subsection{How Important is Reliability?}
\label{sec:org8024643}
Bugs can have large cost with respect to lost revenue and
reputation damage.

Can sacrifice reliability to reduce development cost or
operation cost, but must be conscious of cost cutting.
\section{Scalability}
\label{sec:org96791f9}
\textbf{Scalability}: as the system \emph{grows} (data volume, traffic volume, complexity), there should be reasonable
ways of dealing with growth (cope with increased load)
\subsection{Describing Load}
\label{sec:org55ad82f}
Load can be described with \textbf{load parameters}.

Choice of load parameters depends on the architecture.

Choice of average vs worst case depends on specific system.
\subsection{Describing Performance}
\label{sec:org9c58a31}
Can be helpful to examine:
\begin{itemize}
\item how system performance is affected by increase on load parameter and
keep system resources unchanged
\item how much resources must be increased with load to keep performance unchanged
\end{itemize}

This requires a way to describe performance.

\textbf{Throughput}: number of records that can be processed per time unit

\textbf{Response time}: time between client sending request and receiving a response

\textbf{Latency}: duration that a request is waiting to be handled

Response time can be considered as a distribution.
Within this distribution, good metrics can be average response time or percentiles
like median, 95th percentile, 99th percentile, etc.

\textbf{Tail latencies}: high percentiles of response time, directly affect user
experience

Reducing response times at very high percentiles is difficult since they are
affected by random events outside control and benefits are diminishing.

Percentiles are used in service level objectives and agreements to define
expected performance and availability.

\textbf{Head-of-line blocking}: small number of slow requests holding up subsequent
requests

To simulate queues accurately, generate load independently of response time.

Even if only a small percentage of backend calls are slow, the chance of
getting a slow call increases if an end-user request requires multiple
backend calls, so a higher proportion of end-user requests end up being
slow (\textbf{tail latency amplification}).
\subsection{Approaches for Coping with Load}
\label{sec:org41d1747}
Must rethink architecture on every order of magnitude load increase.

\textbf{Shared-nothing architecture}: distributing load across
multiple machines

\textbf{Elastic system}: can automatically add computing resources when
they detect a load increase (good for unpredictable load)

Stateless services are easy to horizontally scale, stateful
services are difficult to.

Distributed data systems are default.
The way to scale an architecture is specific to the application.
\section{Maintainability}
\label{sec:org5a145f7}
\textbf{Maintainability}: over time, everyone that works on the system (maintaining current behaviour
and adapting system to new use cases) should be able to work on it productively

Main cost in software is ongoing maintenance.
\subsection{Operability: Making Life Easy for Operations}
\label{sec:orgfd97c11}
\textbf{Operability}: make it easy for operations teams to keep system running smoothly

Operations teams:
\begin{itemize}
\item monitor system health
\item track down problem causes
\item keep software up to date
\item keep tabs on system interactions
\item anticipate future problems
\item establish good practices and tools for deployment
\item performance complex maintenance tasks
\item define processes that make operations predictable
\item preserve organization knowledge about the system
\end{itemize}

Good operability means making routine tasks easy, allowing
operations team to focus efforts on high-value activities.
\subsection{Simplicity: Managing Complexity}
\label{sec:org653b215}
\textbf{Simplicity}: make it easy for new engineers to understand the system by removing
as much complexity as possible from the system

As projects get larger, they can become complex and difficult to understand.

Reducing complexity greatly improves the maintainability of software.

Making a system simpler does not necessarily mean reducing functionality,
it can also mean removing accidental complexity (arises only from
implementation).

\textbf{Abstraction} can hide implementation detail behind a clean, simple-to-understand
facade.
\subsection{Evolvability: Making Change Easy}
\label{sec:orgf1fcfe0}
\textbf{Evolvability}: make it easy for engineers to make changes to the system in the
future, adapting it for unanticipated use cases as requirements change

Agile working patterns provide a framework for adapting to change.
These include test-driven development and refactoring.

The ease with which a system can be adapted to changing requirements is
linked to its simplicity and abstractions.
\end{document}
