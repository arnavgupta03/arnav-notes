% Created 2024-04-07 Sun 22:49
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{parskip,darkmode}
\enabledarkmode
\author{Arnav Gupta}
\date{\today}
\title{Multiprocessor, Multicore, and Realtime Scheduling}
\hypersetup{
 pdfauthor={Arnav Gupta},
 pdftitle={Multiprocessor, Multicore, and Realtime Scheduling},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.3 (Org mode 9.7)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Multiprocessor and Multicore Scheduling}
\label{sec:org69940b3}
Multiprocessor systems can be classified as:
\begin{itemize}
\item \textbf{loosely coupled or distributed multiprocessor or cluster}: has collection of relatively
autonomous systems, each processor having its own main memory and IO channels
\item \textbf{functionaly specialized processors}: controlled by the master processor and provide services
to it
\item \textbf{tightly coupled multiprocessor}: has set of processors that share a common main memory and
are under the integrated control of an OS
\end{itemize}
\subsection{Granularity}
\label{sec:org0b3463b}
Multiprocessors can be categorized and placed in context with other architectures by
considering \textbf{synchronization granularity} or frequency between processes in a system.

Grain categories are:
\begin{itemize}
\item \textbf{fine}: parallelism inherent in a single instruction stream, interval <20 instructions
\item \textbf{medium}: parallel processing or multitasking within a single application, interval
20-200 instructions
\item \textbf{coarse}: multiprocessing of concurrent ptocesses in a multiprogramming environment,
interval 200-2000 instructions
\item \textbf{very coarse}: distributed processing across network nodes to form a single computing
environment, interval 2000-1 million instructions
\item \textbf{independent}: multiple unrelated processes, interval not applicable
\end{itemize}
\subsubsection{Independent Parallelism}
\label{sec:org797242c}
No explicit synchronization between processes, each represents a separate job.
Example is time-sharing system.
Works similar to multiprogrammed uniprocessor with shorter average response time.
\subsubsection{Coarse and Very Coarse-Grained Parallelism}
\label{sec:org8909b3c}
Only high-level synchronization among processes so can work as concurrent processes
running on a multiprogrammed uniprocessor, so not much user code change.

Any collection of concurrent processes that need to communicate or synchronize can
benefit from multiprocessor use.
If infrequent process interaction, a distributed system could provide support,
if frequent, then overhead of network communication negates some speedup.
\subsubsection{Medium-Grained Parallelism}
\label{sec:orgabdef84}
If an application is a collection of threads in a process, the programmer must
specify potential parallelism.
Typically, much coordination and interaction among the threads of an application.

Since application threads interact frequently, scheduling decisions concerning 1
thread may affect application performance.
\subsubsection{Fine-Grained Parallelism}
\label{sec:org884dde4}
More complex use of parallelism found in the use of threads.
Specialized and fragmented area.
\subsection{Design Issues}
\label{sec:org699926d}
3 interrelated issues for multiprocessor scheduling:
\begin{enumerate}
\item assignment of processes to processors
\item use of multiprogramming on individual processors
\item actual dispatching a process
\end{enumerate}
\subsubsection{Assignment of Processes to Processors}
\label{sec:orgcbd14b2}
Assume multiprocessor architecture is uniform and treat processors as a pooled
resource and assign processes to processors on demand.

With static assignment, a processor is permanently assinged to one processor from
activation to completion, so a dedicated short-term queue is maintained for each
processor.
Advantage is less overhead in scheduling since processor assignment is known,
allows for gang scheduling.
Disadvantage is uneven load (one processor empty, one has backlog). To fix this,
a global queue can be used.
Context info for all processes will be available to all processors and so the
cost of scheduling will be independent of the identity of the processor on which
it is scheduled.
Another option is dynamic load balancing, where threads are moved from a queue
for one processor to a queue for another processor.

Approaches to assign processes to processors are master/slave and peer.

With master/slave, kernel functions like scheduling are only on a particular
processor (master) and slaves run user programs and call master for service
calls (IO).
Conflict resolution simple since 1 processor has control of memory and IO resources.
Disadvantages are the danger of master failure and bottlenecks from the master.

With peer architecture, any processor can run kernel and each process self-schedules.
Must ensure processes aren't lost and 2 processors do not choose the same process.
\subsubsection{Use of Multiprogramming on Individual Processes}
\label{sec:orged3542c}
When many processors are available, no need for every processor to be as busy as
possible but rather provide best average performance.

An application with many threads may run poorly unless all can run simultaneously.
\subsubsection{Process Dispatching}
\label{sec:org375f03f}
For multiprocessors, priority or scheduling algorithms may have unnecessary overhead.
For thread scheduling, new issues are in play that may be more important than
priorities or execution histories.
\subsection{Process Scheduling}
\label{sec:org97283b9}
Most traditional multiprocessor systems have common pool of processors.

Result of speedup of scheduling algorithms depends on variability in service times,
measured with coefficient of variation \(C_{s}\).
Larger \(C_{s}\) means more variation.
With 2 processors, longer processes do not hold up FCFS too long.

Specific scheduling discipline is less important with 2+ processors than 1.
Simple FCFS (possible with priorities) may work for a multiprocessor system.
\subsection{Thread Scheduling}
\label{sec:orgc22bce9}
On a uniprocessor, threads help with program structure and overlapping IO with
processing.

In a multiprocessing environment, threads allow parallelism leading to dramatic gains
in performance. With medium-grained parallelism, small differences in thread
management and scheduling can have significant performance impact.

General approaches for multiprocessor thread scheduling and processor assignment:
\begin{enumerate}
\item \textbf{load sharing}: global queue of ready threads and each processor selects a thread
from the queue when idle
\item \textbf{gang scheduling}: a set of related threads is scheduled to run on a set of
processors at the same time on a 1:1 basis
\item \textbf{dedicated processor assignment}: threads assigned to processors such that each
thread in a program gets one processor and when program terminates, all processors
returned to general pool for allocation to another program
\item \textbf{dynamic scheduling}: number of threads in a process can be altered during execution
\end{enumerate}
\subsubsection{Load Sharing}
\label{sec:org8bb52ec}
Advantages:
\begin{itemize}
\item load distributed evenly across processors, no unnecessarily idle processor
\item no centralized scheduler needed, scheduling run on available processors
\item global queue organized and accessed using any scheduling policy
\end{itemize}

Versions of load sharing:
\begin{enumerate}
\item \textbf{FCFS}: when a job arrives, each thread placed consecutively at the end of the
shared queue, idle processors pick the next ready thread
\item \textbf{smallest number of threads first}: shared ready queue is organized as a
priority queue with highest priority given to threads from jobs with least
unscheduled threads, if equal then arrival time used
\item \textbf{preemptive smallest number of threads first}: highest priority given to
jobs with least unscheduled threads, and a new job with less threads than
the current can preempt threads belonging to the scheduled job
\end{enumerate}

FCFS is superior since gang scheduling better than load sharing.

Disadvantages of load sharing are:
\begin{itemize}
\item central queue has a region of memory that must enforce mutual exclusion,
which could lead to a bottleneck and could be a problem if not enough
processors
\item preempted threads unlikely to resume execution on the same process, so
caching less efficient
\item unlikely all threads gain access to processors at the same time so much
coordination required
\end{itemize}
\subsubsection{Gang Scheduling}
\label{sec:org7a99a7f}
The concept of scheduling a set of processes simultaneously on a set of
processors.

Advantages:
\begin{itemize}
\item if processes in the group are related or coordinated, synchronization blocking
could be reduced, less process switching, and better performance
\item single scheduling decision affects multiple processors and processes, so less
scheduling overhead
\end{itemize}

\textbf{Coscheduling}: scheduling a related set of tasks (task force) that are small
and close to the idea of a thread

Useful for medium and fine-grained applications where performance degrades
when any part of the application is not running while others are ready
to run.

Gang scheduling reduces process switches since all threads are in system
and saves time in resource allocation since process has IO access.

If there are several single-thread applications, they could all fit together
to increase processor utilization. Otherwise, scheduling could be weighted
by the number of threads.
\subsubsection{Dedicated Processor Assignment}
\label{sec:org8fa7513}
When an application is sceheduled, each thread is assigned a processor
that is dedicated to that thread until application completion.

Wastes time in the case of blocking for IO or synchronization.
Defence of this strategy:
\begin{enumerate}
\item in a highly parallel system, processor utilization is no longer an important
metric for effectiveness or performance
\item avoidance of process switching during program lifetime results in speedup
\end{enumerate}

Performance degrades as both total number of processes exceepts number of
processors.
More threads means more thread preemption and rescheduling, which leads to
inefficiency from many sources.
Limit the number of active threads to the number of processors.

Activity working set is the minimum number of threads that must be scheduled
simultaneously on processors for the application to make acceptable progress.
Not scheduling activity working set could lead to processor thrashing.

\textbf{Processor thrashing}: when scheduling threads whose services are required
deschedules other threads whose services are soon needed

\textbf{Processor fragmentation}: situation in which some processors are left over
when others are allocated and leftovers have insufficient number or lack
organization to support waiting applications

Gang scheduling and dedicated processor allocation avoid these
\subsubsection{Dynamic Scheduling}
\label{sec:org22974e8}
Number of threads in process can be altered dynamically, so OS can adjust
load to improve utilization.

OS partitions processors among jobs and each job executes subset of tasks by
mapping them to threads (subset decided by application).

Here, scheduling responsibility of OS is limited to processor allocation.
When a job requests 1+ processors (when job arrives for the 1st time or
requirements change):
\begin{enumerate}
\item use idle processors first to satisfy request
\item if the requesting job is new, allocate a single processor by taking it from
any job currently allocated 1+ processors
\item if any portion of the request cannot be satisfied, it remains outstanding
until a process becomes available or the job rescinds the request
\item when 1+ processors are released, scan the current queue for unsatisfied
requests and assign a single processor to each job in the list with no
processors, then scan again and allocate based on FCFS
\end{enumerate}

Dynamic allocation superior to gang scheduling and dedicated processor assignment,
but overhead may negate performance advantage.
\subsection{Multicore Thread Scheduling}
\label{sec:org7595825}
Some OSs treat multicore the same as multiprocessor systems (use load balancing).

More cores per chip means minimize access to off-chip memory (with cache and locality)
rather than maximize processor utilization.
This could be complex for caches shared by only some cores.
For this case, scheduling should assign threads that share memory resources to those
that share cache and vice versa for load balancing.

Two aspects of cache sharing:
\begin{itemize}
\item \textbf{cooperative resource sharing}: multiple threads access the same set of main memory
locations
\begin{itemize}
\item data brought into a cache by one thread must be accessed by a cooperating thread,
so cooperating threads should be on adjacent cores (share cache)
\end{itemize}
\item \textbf{resource contention}: when threads compete for cache memory locations
\begin{itemize}
\item if more cache is allocated to one thread, other thread(s) suffer performance
degradation
\item with contention-aware scheduling, allocate threads to cores to maximize
effectiveness of shared cache memory
\end{itemize}
\end{itemize}
\section{Real-Time Scheduling}
\label{sec:org9b383f7}
\subsection{Background}
\label{sec:org1540f2b}
\textbf{Real-time computing}: computing where system correctness depends on logical results and time when
results are produced

Real-time tasks attempt to control or react to events in the outside world, with which it must keep up.

\textbf{Hard real-time task} must meet its deadline or cause unacceptable damage or fatal error to the system

\textbf{Soft real-time task} has an associated deadline that is desirable but not mandatory, can still be
scheduled and completed after its deadline

\textbf{Aperiodic task} has a deadline by which it must finish or start or may have a constraint on
start and finish time

\textbf{Periodic task} has a requirement every \(T\) units apart
\subsection{Characteristics of Real-Time Operating Systems}
\label{sec:org4c48af7}
Have unique requirements in the areas of determinism, responsiveness, user control, reliability,
and fail-soft operation.

An OS is \textbf{deterministic} based on the extent it performs operations at fixed, predetermined times
or within predetermined time intervals.
Impossible when processes compete for resources and in an RTOS, process requests for service are
dictated by external events and timings.
Determinism depends on interrupt response speed and request capacity per time.
One measure of determinism is delay from high priority interrupt to service.

\textbf{Responsiveness} is concerned with how long after acknowledgement it takes an OS to service the
interrupt.
Aspects include:
\begin{enumerate}
\item amount of time required to handle the interrupt and begin execution of the interrupt service
routine (ISR), if this requires a process switch then longer delay
\item amount of time to perform ISR, depends on hardware
\item effect of interrupt nesting, service delayed if ISR can be interrupted
\end{enumerate}

\textbf{User control} is essential in real-time systems to allow users fine-grained control over task priority.
User should be able to distinguish between hard and soft tasks and specify relative priorities for each.
Real-time systems allow the user to specify memory use as well (paging, swapping, resident set, etc).

\textbf{Reliability} is important since real-time systems respond to and control events in real time.
Performance degradation may have catastrophic consequences.

\textbf{Fail-soft operation}: the ability of a system to fail in such a way as to preserve capability and data.

Real-time systems will not simply fail but attempt to correct or minimize effect of failure and continue
to run.
User is notified of corrective action and reduced level of service. If shutdown, data consistency
maintained.

A real-time system is stable if the system at least meets the deadlines of its most critical, highest-priority tasks.

Important features for real-time OSs:
\begin{itemize}
\item a stricter use of priorities with preemptive scheduling designed to meet real-time requirements
\item interrupt latency (time between generation and servicing) bounded and relatively short
\item precise and predictable timing characteristics
\end{itemize}

Design short-term scheduler so that all hard real-time tasks complete (or start) by their deadline and as many as possible
soft real-time tasks complete (or start) by their deadline.
Most RTOSs are designed to be as responsive as possible to real-time tasks so when a deadline approaches, a task can be
quickly scheduled.

Nonpreemptive or preemptive scheduling without priorities does not work for real-time.
Priority preemptive scheduling or immediate preemption could be used instead.
\subsection{Real-Time Scheduling}
\label{sec:org267b3b5}
Scheduling approaches depend on
\begin{enumerate}
\item whether a system performs schedulability analysis
\item if it does, whether it is done statically or dynamically
\item whether the result of analysis produces a schedule or plan according to which tasks are dispatched at runtime
\end{enumerate}

Classes of algorithms:
\begin{itemize}
\item \textbf{static table-driven approaches} perform a static analysis of feasible schedules of dispatching, result is a schedule
that determines at runtime when a task must begin execution
\begin{itemize}
\item used for periodic tasks given periodic arrival time, execution time, periodic deadline, and relative priority
\item predictable and inflexible since changes require new schedule
\end{itemize}
\item \textbf{static priority-driven preemptive approaches} perform a static analysis but no schedule, instead assign priorities to
tasks with a traditional priority-driven preemptive scheduler
\begin{itemize}
\item priority related to time constraints
\end{itemize}
\item \textbf{dynamic planning-based approaches} determine feasibility at runtime rather than statically, arriving task executed only if
feasible to meet time constraints, result is schedule or plan on when to dispatch task
\item \textbf{dynamic best effort approaches} have no feasibility analysis and try to meet all deadlines and abort processes whose deadline
is missed
\begin{itemize}
\item on arrival, a task gets a priority
\item tasks are aperiodic so no static scheduling analysis
\item unknown if deadline will be met until deadline arrives or task complete
\end{itemize}
\end{itemize}
\subsection{Deadline Scheduling}
\label{sec:orge3726ec}
Real-time applications are not concerned with speed but rather completing tasks at the right time despite dynamic resource
demands and conflicts, processing overloads, and hardware or software faults.

Useful info about tasks:
\begin{itemize}
\item \textbf{ready time}: when task becomes ready for execution, known for period and some aperiodic tasks
\item \textbf{starting deadline}: time by which a task must begin
\item \textbf{completion deadline}: time by which a task must be completed
\item \textbf{processing time}: time required to execute task to completion
\item \textbf{resource requirements}: set of resources (other than processor) required by the task
\item \textbf{priority}: relative importance of task, hard real-time tasks could have absolute priority and system failure is missed
\item \textbf{subtask structure}: a task may be decomposed into a mandatory subtask (with hard deadline) and optional subtask
\end{itemize}

A policy of scheduling the task with the earliest deadline minimizes the fraction of tasks that miss their deadlines.
When start deadlines are specified, use nonpreemptive scheduler (task blocks itself).
When completion deadlines are specifies, use preemptive scheduler (OS preempts).

Straightforward scheme is to always schedule the ready task with earliest deadline and let it run to completion.
For aperiodic tasks, this could lead to missed deadlines.
Instead, earliest deadline with unforced idle times could be used where the task with the earliest deadline is scheduled
and run to completion, even if processor is idle.
\subsection{Rate Monotonic Scheduling}
\label{sec:org0a955a8}
Assigns priorities to tasks on the basis of their periods, highest priority is shortest period.

Period \(T\) is the amount of time between arrival of one instance of the task and arrival of the next instance of the task.
Rate is the inverse of period.
Execution time \(C\) is the amount of processing time required for each occurrence where \(C \le T\) for uniprocessor.
If task never denied service, then processor utilization by task is \(U = C/T\).

To check if all hard deadlines are met, for \(n\) tasks to meet all deadlines, the following must hold (for a perfect
scheduling algorithm):
$$
\frac{C_{1}}{T_{1}} + \frac{C_{2}}{T_{2}} + \cdots + \frac{C_{n}}{T_{n}} \le 1
$$
For RMS, this is
$$
\frac{C_{1}}{T_{1}} + \frac{C_{2}}{T_{2}} + \cdots + \frac{C_{n}}{T_{n}} \le n(2^{1/n} - 1)
$$
which converges to \(\ln(2) \approx 0.693\) for infinite tasks.
If total utilization is less than this bound, then all tasks will be successfully scheduled.

For earliest-deadline scheduling, upper bound is 1, so more processor utilization and more periodic tasks accommodated.

Benefits of RMS:
\begin{enumerate}
\item performance difference is small in practice, as 0.693 is conservative
\item most hard real-time systems have soft real-time components that can execute at lower-priorities when not running
RMS scheduling for hard real-time tasks
\item RMS more stable, since essential tasks must have deadlines guaranteed if schedulable, which can be done in RMS by
giving them short periods or modifying RMS priorities to account for essential tasks
\begin{enumerate}
\item with earliest deadline, a periodic task's priority changes every period so less guarantees
\end{enumerate}
\end{enumerate}
\subsection{Priority Inversion}
\label{sec:org9d0dd6f}
\textbf{Priority inversion} occurs when circumstances within the system force a higher-priority task to wait for
a lower-priority task.
\begin{itemize}
\item could occur if a higher-priority task attempts to lock a resource a lower-priority task already has
\end{itemize}

\textbf{Unbounded priority inversion}: when the duration of a priority inversion depends not only on the
time required to handle a shared resource but also on the unpredictable actions of other
unrelated tasks.
\subsubsection{Priority Inheritance}
\label{sec:org77cc164}
A lower-priority task inherits the priority of any higher-priority task pending on a shared resource.

The priority change occurs as soon as the higher-priority task blocks on the resource, ends when
the resource is released by the lower-priority task.
\subsubsection{Priority Ceiling}
\label{sec:org246135c}
A priority is associated with each resource, where the priority assigned to a resource is 1 level
higher than the priority of its highest-priority user.

The scheduler dynamically assigns this priority to any task that accesses that resource, and when
the task finishes with the resource, priority returns to normal.
\end{document}
