% Created 2024-11-27 Wed 10:20
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{parskip,darkmode}
\usepackage{algorithm}
\usepackage{algpseudocode}
\enabledarkmode
\author{Arnav Gupta}
\date{\today}
\title{Supervised Machine Learning}
\hypersetup{
 pdfauthor={Arnav Gupta},
 pdftitle={Supervised Machine Learning},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.7.11)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Linear Regression}
\label{sec:org988c30e}
A model in which the output is a linear function of the input features:
$$ \hat{Y}_{\vec{w}}(e) = \sum_{i=0}^{n} w_{i}X_{i}(e) $$
where \(\vec{w} = \left< w_{0}, \dots, w_{n} \right>\) and \(X_{0} = 1\).

The \textbf{sum of squares error} on examples \(E\) for output \(Y\) is
$$ \text{Error}(E, \vec{w}) = \sum_{e \in E} (Y(e) - \hat{Y}_{\vec{w}}(e))^{2} $$

The goal is to find weights that minimize the sum of squares error.

The minimum error can be found analytically.
For output features of \(M\) examples \(\vec{y}\), \(X\) being a matrix with column \(j\)
holding the values of the input features for example \(j\), and weights \(\vec{w}\),
$$ \vec{y} = \vec{w} X \implies \vec{y} X^{T} (XX^{T})^{-1} = \vec{w} $$

The minimum error can also be found iteratively, which works for more problems.
This uses \textbf{gradient descent}:
$$ w_{i} \gets w_{i} - \eta \frac{\partial \text{Error}(E, \vec{w})}{\partial w_{i}} $$
where \(\eta\) is the gradient descent step size, called the \textbf{learning rate}.
Using sum of squares error here gives the rule
$$ w_{i} \gets w_{i} + \eta \sum_{e \in E} \left( Y(e) - \sum_{i=0}^{n} w_{i} X_{i}(e) \right) X_{i}(e) $$
where \(\eta \to 2\eta\) is an arbitrary scale.

\begin{algorithm}
\caption{LinearLearner}
\begin{algorithmic}[1]
\State \textbf{Input:} Set of input features, $X = \{X_1, \dots, X_n\}$
\State \textbf{Input:} Output feature, $Y$
\State \textbf{Input:} Set of examples, $E$
\State \textbf{Input:} Learning rate, $\eta$
\State Initialize weights $\vec{w} = \langle 0, 0, \dots, 0 \rangle$ randomly
\Repeat
    \For{each example $e \in E$}
        \State $\delta \gets Y(e) - \sum_{i=0}^{n} w_i X_i(e)$
        \For{each $i \in \{0, \dots, n\}$}
            \State $w_i \gets w_i + \eta \delta X_i(e)$
        \EndFor
    \EndFor
\Until{Stop criteria are true}
\end{algorithmic}
\end{algorithm}

With this algorithm, if examples are chosen at random on line 7, this becomes
\textbf{stochastic gradient descent}.

With \textbf{batched gradient descent}:
\begin{enumerate}
\item process a batch of size \(n\) before updating weights
\item if \(n\) is all the data, this is gradient descent
\item if \(n=1\), this is incremental gradient descent
\end{enumerate}

Incremental can be more efficient than batch, but it is not guaranteed to
converge.
This is because weight updates are done immediately but this might undo the
work of other weight updates, creating an oscillation.
\section{Linear Classifier}
\label{sec:orgff51065}
For binary classification between 0 and 1, the \textbf{squashed linear function} is of the form:
$$ \hat{Y}_{\vec{w}}(e) = f \left( \sum_{i=0}^{n} w_{i}X_{i}(e) \right) $$
where \(f\) is an \textbf{activation function}.

If the activation function is differentiable, gradient descent can be used to update
the weights.
The partial derivative of the sum of squares error for an example \(e\) becomes
$$ \frac{\partial \text{Error}(e, \vec{w})}{\partial w_{i}} = -2 \delta f' \left( \sum_{i} w_{i} X_{i}(e) \right) X_{i}(e) $$
where \(\delta\) is the square root of the sum of squares error.
Thus, each example \(e\) updates each weight \(w_{i}\) by
$$ w_{i} \gets w_{i} + \eta \delta f' \left( \sum_{i} w_{i}X_{i}(e) \right) X_{i}(e) $$

The logistical activation function is
$$ f(x) = \frac{1}{1 + e^{-x}}, \; f'(x) = f(x)(1 - f(x)) $$
\section{Neural Networks}
\label{sec:org5ef0b17}
Inspired by networks in the brain.
Idea is to connect simple units that have a threshold and fire.

Neural networks can learn the same things as a decision tree but imposes
different learning bias.
Linear and sigmoid layers are treated as a single layer.

\textbf{Backpropagation learning}: errors made are propagated backwards to change the weights

Each node \(j\):
\begin{itemize}
\item has a set of weights \(w_{j} = [w_{j0}, \dots, w_{jN}]\)
\item receives inputs \(v = [v_{0}, \dots, v_{N}]\) which are either example features
\([x_{0}, \dots, x_{N}]\) or the output of a previous layer \(\ell\)
\([z^{(\ell)}_{0}, \dots, z^{(\ell)}_{N}]\).
\end{itemize}

The number of weights is one more than the number of parents.
The output is the activation function output
$$ z_{j} = f \left( \sum_{i} w_{ij} v_{i} \right) $$
which is necessarily non-linear.

When connecting neurons into a network:
\begin{itemize}
\item \textbf{feedforward network}
\begin{itemize}
\item forms a directed acyclic graph
\item have connections only in one direction
\item represents a function of its inputs
\end{itemize}
\item \textbf{recurrent network}
\begin{itemize}
\item feeds outputs back into inputs
\item supports short-term memory, so for given inputs, the behaviour of the network
depends on the initial state, which may depend on previous inputs
\end{itemize}
\end{itemize}
\subsection{Activation Functions}
\label{sec:orgf585f5a}
\subsubsection{Step Function}
\label{sec:org13c26fb}
\(f(x) = 1\) if \(x > 0\) else \(f(x) = 0\).
Simple to use but not differentiable so not used in practice.
\subsubsection{Sigmoid Function}
\label{sec:orge442606}
$$ f(x) = \frac{1}{1 + e^{-kx}} $$
For very large or small \(x\), \(f(x)\) approaches 1 or 0.
Tuning \(k\) approximates the step function, with higher values of \(k\)
leading to a steeper sigmoid.
Sigmoid is differentiable but computationally expensive.

\textbf{Vanishing gradient problem}: when \(x\) is very large/small, \(f(x)\)
responds very little to changes in \(x\), so the network does not learn
further or learns very slowly.
\subsubsection{Rectified Linear Unit (ReLU)}
\label{sec:orgcf200ea}
$$ f(x) = \max (0, x) $$
Computationally efficient and network converges quickly.
ReLU is differentiable.

\textbf{Dying ReLU problem}: when inputs approach 0 or are negative,
the gradient becomes 0 and the network cannot learn.
\subsubsection{Leaky ReLU}
\label{sec:org4b2da47}
$$ f(x) = \max(0, x) + k \cdot \min(0, x) $$
Small positive slope \(k\) in the negative area enables learning for
negative input values.
\subsection{Backpropagation}
\label{sec:org5613265}
\textbf{Backpropagation} implements stochastic gradient descent.

The \textbf{backpropagation algorithm} is an efficient method of calculating the gradients
in a multi-layer neural network.
Given training examples \((\vec{x}_{n}, \vec{y}_{n})\) and an error/loss function
\(\text{Error}(\hat{Y}, Y)\), perform 2 passes:
\begin{itemize}
\item \textbf{forward pass}: compute the error given the inputs and weights
\item \textbf{backward pass}: compute the gradients \(\frac{\partial \text{Error}}{\partial W_{j, k}^{(2)}}\)
and \(\frac{\partial \text{Error}}{\partial W_{i, j}^{(1)}}\)
\end{itemize}

Update each weight by the sum of the partial derivatives for all the training examples.
\subsection{Generalization and Optimization}
\label{sec:org4f3a328}
For unit \(j\) of layer \(\ell\)
$$ \delta_{j}^{(\ell)} = \begin{cases} \frac{\partial \text{Error}}{\partial z_{j}^{(\ell)}} \times f'(a_{j}^{(\ell)}) & \text{base case, $j$ is an output unit} \\ \left( \sum_{k} \delta_{k}^{(\ell + 1)} W_{j, k}^{(\ell + 1)} \right) \times f'(a_{j}^{(\ell)}) & \text{recursive case, $j$ is a hidden unit} \end{cases} $$

Using this, update weights using
$$ W_{j, k}^{(\ell)} \gets W_{j, k}^{(\ell)} - \eta \frac{\partial \text{Error}}{\partial W_{j, k}^{(\ell)}} $$
where \(\eta\) is the learning rate
$$ \frac{\partial \text{Error}}{\partial W_{j, k}^{(\ell)}} = \delta_{k}^{(\ell)} z_{j}^{(\ell - 1)} $$
for all layers beyond the input layer, or
$$ \frac{\partial \text{Error}}{\partial W_{j, k}^{(\ell)}} = \delta_{k}^{(1)} x_{j} $$
at the input layer.

To improve optimization:
\begin{itemize}
\item \textbf{momentum}: weight changes accumulate over iterations
\item \textbf{RMS-prop}: rolling average of square of gradient
\item \textbf{Adam}: combination of momentum and RMS-prop
\item \textbf{initialization}: randomly set params to start
\end{itemize}

\textbf{Regularized Neural nets} prevent overfitting, increased bias for reduced variance through:
\begin{itemize}
\item param norm penalties added to the objective function
\item dataset augmentation
\item early stopping
\item dropout
\item param tying
\begin{itemize}
\item \textbf{convolutional} neural nets: used for images, params tied across space
\item \textbf{recurrent} neural nets: used for sequences, params tied across time
\end{itemize}
\end{itemize}
\subsection{Modeling}
\label{sec:orgc2cb67c}
\subsubsection{Sequence Modeling}
\label{sec:org0bc7c54}
\textbf{Word Embedding}: latent vector spaces that represent the meaning of words in context

\textbf{RNNs}: neural net repeats over time and has inputs from previous time step

\textbf{LSTM}: RNN with longer-term memory

\textbf{Attention}: uses expected embeddings to focus updates on relevant parts of the network

\textbf{Transformers}: multiple attention mechanisms

\textbf{LLMs}: very large transformers for language
\subsubsection{Composite Models}
\label{sec:org442526d}
In \textbf{random forests}, each decision tree in the forest is different, with different features,
splitting criteria, and training sets.
The average or majority vote determines the output.

In \textbf{support vector machines}, find the classification boundary with the widest margin.
Combine this with the kernel trick.

\textbf{Ensemble learning} is a combination of base-level algorithms.

In \textbf{boosting}, a sequence of learners fit the examples the previous learner did not fit well.
This way, learners are progressively biased towards higher precision.
Early learners have many false positives but reject all clear negatives.
Late learners have a more difficult problem, but the set of examples is more focused.
\end{document}
