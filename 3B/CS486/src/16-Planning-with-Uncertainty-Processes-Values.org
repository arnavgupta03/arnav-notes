#+title: Planning With Uncertainty: Processes and Values
#+author: Arnav Gupta
#+LATEX_HEADER: \usepackage{parskip,darkmode}
#+LATEX_HEADER: \enabledarkmode
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="src/latex.css" />

* Worlds and Planning
Agents carry out actions according to the *horizon*:
- _infinite horizon_: forever
- _indefinite horizon_: until stopping criteria is met
- _finite horizon_: finite and fixed number of steps

It is helpful to know, what an agent should do when (for all planning horizons):
- it gets rewards/punishments and tries to maximize rewards
- actions can be noisy, so the outcome can't be fully predicted
- there is a model that specifies the probabilistic outcome of actions
- the world is fully observable: current state is always fully in evidence

** World State
The information such that if the *world state* is known, no info about
the past is relevant to the future.

The *Markovian Assumption* is: Let $S_{i}, A_{i}$ be the state and action at
time $i$, then
$$ P(S_{t+1} \mid S_{0}, A_{0}, \dots, S_{t}, A_{t}) = P(S_{t+1} \mid S_{t}, A_{t}) $$
where $P(s' \mid s, a)$ is the probability that the agent will be in state $s'$
immediately after doing action $a$ in state $s$.

The dynamics is *stationary* is the distribution is the same for each time point.

If the process never halts, this has an infinite horizon.

If the process stays in a state getting no reward, these are *absorbing states*,
which has an indefinite horizon.

* Markov Decision Processes
Augments a Markov chain with actions and values.

For an MDP, specify the:
- set $S$ of states
- set $A$ of actions
- $P(S_{t+1} \mid S_{t}, A_{t})$ specifies the dynamics
- $R(S_{t}, A_{t}, S_{t+1})$ specifies the *reward*, which the agent gets at
  each time step, specifically when it ends up in $S_{t+1}$ after doing $A_{t}$
  from $S_{t}$

*Fully-observable MDP*: agent gets to observe $S_{t}$ when deciding on action $A_{t}$

*Partially-observable MDP (POMDP)*: agent has some noisy sensor of the state, so it
must remember sensing and acting history by maintaining a sufficiently complex
*belief state*

** Rewards and Values
Suppose the agent receives the sequence of rewards $r_{1}, r_{2}, \dots$.
The value that should be assigned could be:
- _total reward_
  $$ V = \sum_{i=1}^{\infty} r_{i} $$
- _average reward_
  $$V = \lim_{n \to \infty} (r_{1} + \cdots + r_{n}) / n$$
- _discounted reward_
  $$V = r_{1} + \gamma r_{2} + \gamma^{2} r_{3} + \cdots$$
  where $0 \le \gamma \le 1$ is the *discount factor*

** Policies
A *stationary policy* is a function $\pi : S \to A$ where for a given state $s$,
$\pi(s)$ specifies what action the agent who is following $\pi$ will do.

*Optimal policy*: one with maximum expected discounted reward

For a fully-observable MDP with stationary dynamics and rewards with infinite
or indefinite horizon, there is always an optimal stationary policy.

*** Policy Value
$Q^{\pi}(s, a)$ for some action $a$ and state $s$, is the expected value of
doing $a$ in state $s$, then following policy $\pi$.

$V^{\pi}(s)$ for some state $s$, is the expected value of following policy
$\pi$ in state $s$.

$Q^{\pi}$ and $V^{\pi}$ are *mutually recursive*:
$$ Q^{\pi}(s,a) = \sum_{s'} P(s' \mid a, s) ( r(s, a, s') + \gamma V^{\pi}(s') ) $$
$$ V^{\pi}(s) = Q^{\pi}(s, \pi(s)) $$

*** Optimal Policy Value
$Q^{*}(s, a)$ for some action $a$ and state $s$, is the expected value of
doing $a$ in state $s$, then following the optimal policy.

$\pi^{*}(s)$ is the optimal action to take in state $s$.

$V^{*}(s)$ for some state $s$, is the expected value of following the optimal
policy in state $s$.

$Q^{\*}$ and $V^{\*}$ are *mutually recursive*.
Further:
$$ \pi^{\*}(s) = \text{arg} \max_{a} Q^{\*}(s,a) $$

* Value Iteration
*t-step lookahead value function* $V^{t}$: expected value with $t$ steps to go

The goal, given an estimate of the $t$ step lookahead value function, is to
determine the $t+1$ step lookahead value function.

** Steps
Set $V^{0}$ arbitrarily and $t = 1$.
Compute $Q^{t}$ and $V^{t}$ from $V^{t-1}$:
$$ Q^{t}(s,a) = \left[ R(s) + \gamma \sum_{s'} \text{Pr}(s' \mid s, a) V^{t-1} (s') \right] $$
$$ V^{t}(s) = \max_{a} Q^{t} (s,a) $$

The policy with $t$ stages to go is simply the action that maximizes the following
$$ \pi^{t}(s) = \text{arg} \max_{a} [R(s) + \gamma \sum_{s'} \text{Pr}(s' \mid s, a) V^{t-1}(s')] $$
This converges exponentially fast over $t$ to the optimal value function.

Let $\| X \| = \max \{ |x|, x \in X \}$.
Convergence when $\| V^{t}(s) - V^{t-1}(s) \| < \epsilon \frac{(1-\gamma)}{\gamma}$ ensures
$V^{t}$ is within $\epsilon$ of the optimal.

** Asynchronous Value Iteration
Can update value function for each state individually rather than sweeping through all states.
This converges to the optimal value function if each state and action are visited
infinitely often in the limit.
Either $V[s]$ or $Q[s, a]$ can be stored.

To store $V[s]$, repeat the following forever:
1. select state $s$
2. $V[s]$ becomes
   $$ \max_{a} \sum_{s'} P(s' \mid s, a) (R(s, a, s') + \gamma V[s']) $$
3. select action $a$ (using an exploration policy)

To store $Q[s,a]$, repeat the following forever:
1. select state $s$ and action $a$
2. $Q[s, a]$ becomes
   $$\sum_{s'} P(s' \mid s, a) \left( R(s, a, s') + \gamma \max_{a'} Q[s', a'] \right) $$

* Markov Decision Processes and State
Represent $S = \{X_{1}, \dots, X_{n}\}$ where $X_{i}$ are random variables.
For each $X_{i}$ and each action $a \in A$, there is $P(X_{i}' \mid S, A)$.

The reward may be additive:
$$ R(X_{1}, \dots, X_{N}) = \sum_{i} R(X_{i}) $$

Value iteration proceeds as usual but can do one variable at a time, like
variable elimination.

A *Partially Observable Markov Decision Process (POMDP)* is like an MDP but
some variables are not observed.
It is a tuple $\left< S, A, T, R, O, \Omega \right>$:
- $S$ is a finite set of unobservable states
- $A$ is a finite set of agent actions
- $T : S \times A \to S$ is a transition function
- $R : S \times A \to \mathcal{R}$ is a reward function
- $O$ is a set of observations
- $\Omega : S \times A \to O$ is an observation function
