#+title: Decision Trees And Training Strategies
#+author: Arnav Gupta
#+LATEX_HEADER: \usepackage{parskip,darkmode}
#+LATEX_HEADER: \enabledarkmode
#+LATEX_HEADER: \usepackage{algorithm,algpseudocode}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{trees, arrows.meta}
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="src/latex.css" />

* Decision Trees
Technique for supervised learning from discrete data:
- representation is a decision tree
- bias towards simple decision tree
- search through space of decision trees, from simple to complex

For each *decision tree*:
- *nodes*: input attributes/features
- *branches*: labeled with input feature values (can have multiple feature values on one branch)
- *leaves*: predictions for target features (point estimates)

#+BEGIN_SRC latex
\begin{center}
\begin{tikzpicture}[every node/.style={draw, rectangle, rounded corners, align=center, minimum width=2.5cm},
                    level distance=1.5cm,
                    sibling distance=5.5cm] % Increase sibling distance
    % Root node
        \node[fill=blue!80!black!60] {Go for a hike?}
        child { node[fill=blue!80!black!60] {Is it raining?}
            child { node[fill=green!60!black!100] {Yes}
                child { node[fill=red!60!black!80] {Stay indoors} } % Leaf
            }
            child { node[fill=green!60!black!100] {No}
                child { node[fill=blue!80!black!60] {Is it too hot?}
                    child { node[fill=green!60!black!100] {Yes}
                        child { node[fill=red!60!black!80] {Go to the beach} } % Leaf
                    }
                    child { node[fill=green!60!black!100] {No}
                        child { node[fill=red!60!black!80] {Go for a hike} } % Leaf
                    }
                }
            }
        };
\end{tikzpicture}
\end{center}
#+END_SRC

To learn a decision tree:
1. split training data based some criteria (bias)
2. recursively solve sub-problems

Criteria for good decision trees: small, good classification (low error on training data), good generalization (low error on test data).

#+BEGIN_SRC latex
\begin{algorithm}
\caption{Decision Tree Learner}
\begin{algorithmic}[1]
\Procedure{DecisionTreeLearner}{X, Y, E}
    \If{stopping criteria is met}
        \State \Return pointEstimate(Y, E)
    \Else
        \State select input feature \( X_i \in X \)
        \For{each value \( x_i \) of \( X_i \)}
            \State \( E_i \gets \) all examples in \( E \) where \( X_i = x_i \)
            \State \( T_i \gets \text{DecisionTreeLearner}(X \setminus \{X_i\}, Y, E_i) \)
        \EndFor
        \State \Return \( \langle X_i, T_1, \ldots, T_N \rangle \)
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
#+END_SRC

#+BEGIN_SRC latex
\begin{algorithm}
\caption{Classify Example}
\begin{algorithmic}[1]
\Procedure{ClassifyExample}{e, X, Y, DT}
    \State \( S \gets DT \)
    \While{S is an internal node of the form \( \langle X_i, T_1, \ldots, T_N \rangle \)}
        \State \( j \gets X_i(e) \)
        \State \( S \gets T_j \)
    \EndWhile
    \State \Return S
\EndProcedure
\end{algorithmic}
\end{algorithm}
#+END_SRC

** Stopping Criteria
Related to the final return value, depends on what must be done.

Possible stopping criteria:
- no more features
- performance on training data good enough

** Feature Selection
Choose sequence of features that result in smallest tree.

In practice, split based on what gives best performance.

Heuristics for best performing feature:
- most even split
- max info gain
- GINI index

* Information Theory
$n$ bits can distinguish $2^{n}$ items, but can do better with probabilities.

In general, need $-\log_{2} P(x)$ bits to encode $x$.
Each symbol requires on average
$$
-P(x)\log_{2} P(x) \text{ bits}
$$

To transmit an entire sequence distributed according to $P(x)$,
$$
\sum_{x} -P(x)\log_{2} P(x) \text{ bits}
$$
bits of info per symbol are needed on average, which is the *info content* or *entropy* of the sequence.

** Information Gain
Given a set $E$ of $N$ training examples, if the number of examples with output feature $Y = y_{i}$
is $N_{i}$, then
$$
P(Y = y_{i}) = P(y_{i}) = \frac{N_{i}}{N}
$$
is the point estimate.

The total info content for $E$ is
$$
I(E) = - \sum_{y_{i} \in Y} P(y_{i}) \log( P(y_{i}) )
$$
After splitting $E$ into $E_{1}$ and $E_{2}$ based on input feature $X_{i}$, the information
content is
$$
I(E_{split}) = \frac{N_{1}}{N} I(E_{1}) + \frac{N_{2}}{N} I(E_{2})
$$
so the desirable $X_{i}$ is the one that maximizes *info gain*:
$$
I(E) - I(E_{split}) = - \sum_{y \in Y} P(y) \log P(y) + \sum_{x \in X, y \in Y}P(x,y) \log  \frac{P(x,y)}{P(x)}
$$
which gives the inequality
$$
IG = - \sum_{x \in X, y \in Y} P(x,y) \log \frac{P(x) P(y)}{P(x,y)} \ge -\log \left( \sum_{x \in X, y \in Y} P(x, y) \frac{P(x)P(y)}{P(x, y)} \right)
$$

Info gain is the reduction in uncertainty about the output feature $T$ given the value
of a certain input feature $X$.

*Jensen's inequality*: for a convex function $f(x)$, $E[f(x)] \ge f(E[x])$.

* Training Strategies
** Final Return Value
Point estimate (prediction of target features) of $Y$ over all examples.

A point estimate could be mean, median, mode, or
$$
P(Y = y_{i}) = \frac{N_{i}}{N}
$$

** Priority Queue
Sort leaves using a priority queue ranked by how much info can be gained with the best feature at that
leaf.
Always expand the leaf at the top of the queue.

#+BEGIN_SRC latex
\begin{algorithm}
\caption{Decision Tree Learner}
\begin{algorithmic}[1]
\Procedure{DecisionTreeLearner}{X, Y, E}
    \State \( DT \gets \text{pointEstimate}(Y, E) \) \Comment{initial decision tree}
    \State \( \{ X', \Delta I \} \gets \text{best feature and Information Gain value for } E \)
    \State \( PQ \gets \{ (DT, E, X', \Delta I) \} \) \Comment{priority queue of leaves ranked by \( \Delta I \)}
    \While{stopping criteria is not met}
        \State \( \{ S_\ell, E_\ell, X_\ell, \Delta I_\ell \} \gets \text{leaf at the head of } PQ \)
        \For{each value \( x_i \) of \( X_\ell \)}
            \State \( E_i \gets \) all examples in \( E_\ell \) where \( X_\ell = x_i \)
            \State \( \{ X_j, \Delta I_j \} \gets \text{best feature and value for } E_i \)
            \State \( T_i \gets \text{pointEstimate}(Y, E_i) \)
            \State insert \( \{ T_i, E_i, X_j, \Delta I_j \} \) \text{ into } \( PQ \text{ according to } \Delta I_j \)
        \EndFor
        \State \( S_\ell \gets \langle X_\ell, T_1, \ldots, T_N \rangle \)
    \EndWhile
    \State \Return \( DT \)
\EndProcedure
\end{algorithmic}
\end{algorithm}
#+END_SRC

** Overfitting
When there is not enough data, the decision tree does not generalize to test data.

Methods to avoid overfitting:
- *regularization*: prefer small decision trees, so add a complexity penalty to stopping criteria
- *pseudocounts*: add data based on prior knowledge
- *cross validation*

#+BEGIN_SRC latex
\begin{center}
\begin{tikzpicture}[scale=0.6] % Adjust scale for better visibility
    % Axes
    \draw[->] (0,0) -- (10,0) node[right] {Tree Size};
    \draw[->] (0,0) -- (0,10) node[above] {\% Correct};

    % Training data points (increasing accuracy with tree size)
    \foreach \x in {1,2,...,5} {
        \fill[blue!80!black!40] (\x, {1.7 * \x}) circle (2pt); % Training accuracy increases
    }

    % Test data points (accuracy increases then decreases)
    \foreach \x in {1,2,...,9} {
        \fill[red!60!black!50] (\x, {7 - 0.15 * (\x - 7)^2}) circle (2pt); % Test accuracy rises then falls
    }

    % Training line
    \draw[blue!80!black!40, thick] plot[domain=0:5] (\x, {1.7 * \x}) node[right] {Training Accuracy};

    % Test line (parabolic decline)
    \draw[red!60!black!50, thick, dashed] plot[domain=0.15:10] (\x, {7 - 0.15 * (\x - 7)^2}) node[right] {Test Accuracy};

    % Legend
    \draw[blue!80!black!40, thick] (8, 11) -- (9, 11) node[right] {Training Data};
    \draw[red!60!black!50, thick, dashed] (8, 10) -- (9, 10) node[right] {Test Data};
\end{tikzpicture}
\end{center}
#+END_SRC

Test set errors are caused by:
- *bias*: error due to the algorithm finding an imperfect model
  - *representation bias*: model too simple
  - *search bias*: not enough search
- *variance*: error due to lack of data
- *noise*: error due to data depending on features not modeled or because process generating
  data is stochastic
- *bias-variance trade-off*
  - complicated model, not enough data
  - simple model, lots of data

*Capacity*: ability of a model to fit a wide variety of functions (inverse of bias)

*Cross Validation*:
- split training data into training and validation set
- use validation set as fake test set
- optimize decision maker to perform well on validation set
- repeat with different validation sets
