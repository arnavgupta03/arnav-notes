% Created 2024-10-29 Tue 19:41
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{parskip,darkmode}
\enabledarkmode
\author{DESKTOP-H800RKQ}
\date{\today}
\title{More Bayesian Networks}
\hypersetup{
 pdfauthor={DESKTOP-H800RKQ},
 pdftitle={More Bayesian Networks},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.7.11)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Bayesian/Belief Networks}
\label{sec:org7a2eb1f}
To represent a domain in a belief network, consider:
\begin{itemize}
\item what the relevant variables are
\begin{itemize}
\item what might be observed \(\to\) \textbf{evidence}
\item what information is desired \(\to\) \textbf{query}
\item what features make the model simpler \(\to\) other variables
\end{itemize}
\item what values the variables will take
\item what relationships exist between variables, expressed in terms of \textbf{local influence}
\item how the values of each variable depend on the parents, expressed in terms of conditional probabilities
\end{itemize}
\subsection{Bayes' Rule}
\label{sec:orgb947e83}
Agent has a prior belief in a hypothesis \(h\), \(P(h)\).

Agent observes some evidence \(e\) that has a \textbf{likelihood} given the hypothesis \(P(e \mid h)\).

The agent's \textbf{posterior belief} about \(h\) after observing \(e\), \(P(h \mid e)\)
is given by Bayes' Rule
$$ P(h \mid e) = \frac{P(e \mid h) P(h)}{P(e)} = \frac{P(e \mid h) P(h)}{\sum_{h} P(e \mid h) P(u)} $$

Often agents have \textbf{causal knowledge} but want to do \textbf{evidential reasoning}.
\subsection{Simple Forward Inference (Chain)}
\label{sec:orgd948140}
Computing marginal requires simple forward propagation of probabilities.

Consider the following Bayesian Network:

\textbf{Marginalization}
$$ P(B) = \sum_{m, c} P(M = m, C = c, B) $$

\textbf{Chain Rule}
$$ P(B) = \sum_{m, c} P(B \mid m, c) P(m \mid c) P(c) $$

\textbf{Independence}
$$ P(B) = \sum_{m, c} P(B \mid m, c) P(m) P(c) $$

\textbf{Distribution of Produce over Sum}
$$ P(B) = \sum_{m} P(m) \sum_{c} P(c) P(B \mid m, c) $$

All terms on the last line are CPTs in the Bayes' Network.
Only ancestors of \(B\) are considered.

With multiple parents, the evidence can be pooled.
This also works with upstream evidence.
\subsection{Simple Backward Inference}
\label{sec:orgb5e6e90}
When evidence is downstream of query, reason backwards, which requires Bayes' rule.
\section{Variable Elimination}
\label{sec:org95178fb}
Intuitions above are the \textbf{polytree} algorithm.
This works for simple networks without loops.

A more general algorithm is \textbf{Variable Elimination}:
\begin{itemize}
\item applies sum-out rule repeatedly
\item distributes sums
\end{itemize}

\textbf{Factor}: representation of a function from a tuple of random variables into a number

Write a factor \(f\) on variables \(X_{1}, \dots, X_{j}\) as \(f(X_{1}, \dots, X_{j})\).
Assigning some or all of the variables of a factor is \textbf{restricting} a factor:
\begin{itemize}
\item \(f(X_{1} = v_{1}, X_{2}, \dots, X_{j})\) where \(v_{1} \in \text{dom}(X_{1})\) is a factor on
\(X_{2}, \dots, X_{j}\)
\item \(f(X_{1} = v_{1}, X_{2} = v_{2}, \dots, X_{j} = v_{j})\) is a number that is the value of \(f\)
when each \(X_{i}\) has value \(v_{i}\)
\end{itemize}

The \textbf{product} of factor \(f_{1} (X,Y)\) and \(f_{2} (Y,Z)\) where \(Y\) are the variables in common,
is the factor \((f_{1} \times f_{2}) (X,Y,Z)\) defined by
$$ (f_{1} \times f_{2}) (X,Y,Z) = f_{1} (X,Y) f_{2} (Y,Z) $$

To \textbf{sum out} a variable \(X_{1}\) with domain \(\{ v_{1}, \dots, v_{k} \}\), from factor
\(f(X_{1}, \dots, X_{j})\) resulting in a factor on \(X_{2}, \dots, X_{j}\) defined by:
$$ \left( \sum_{X_{1}} f \right) (X_{2}, \dots, X_{j}) = f(X_{1} = v_{1}, \dots, X_{j}) + \cdots + f(X_{1} = v_{k}, \dots, X_{j}) $$
\subsection{Evidence}
\label{sec:org2bbe6d9}
To compute the posterior probability of \(Z\) given evidence
\(Y_{1} = v_{1} \wedge \cdots \wedge Y_{j} = v_{j}\):
$$ P(Z \mid Y_{1} = v_{1}, \dots, Y_{j} = v_{j}) = \frac{P(Z, Y_{1} = v_{1}, \dots, Y_{j} = v_{j})}{\sum_{Z} P(Z, Y_{1} = v_{1}, \dots, Y_{j} = v_{j})} $$

This computation reduces to the joint probability of \(P(Z, Y_{1} = v_{1}, \dots, Y_{j} = v_{j})\)
normalized at the end.

The query value can also be restricted, for example \(Z = z\).
\subsection{Probability of a Conjunction}
\label{sec:org77753e9}
Suppose the variables of the belief network are \(X_{1}, \dots, X_{n}\).
To compute \(P(Z, Y_{1} = v_{1}, \dots, Y{j} = v_{j})\), sum out the variables other than query \(Z\)
and evidence \(Y\)
$$ Z_{1}, \dots, Z_{k} = \{ X_{1}, \dots, X_{n} \} - \{ Z \} - \{ Y_{1}, \dots, Y_{j} \} $$

Order \(Z_{i}\) into an elimination ordering \(Z_{1}, \dots, Z_{k}\)
$$ P(Z, Y_{1} = v_{1}, \dots, Y_{j} = v_{j}) = \sum_{Z_{k}} \cdots \sum_{Z_{1}} \prod_{i=1}^{n} P(X_{i} \mid \text{parents}(X_{i}))_{Y_{1} = v_{1}, \dots, Y_{j} = v_{j}} $$
\subsection{Computing Sums of Products}
\label{sec:orgf98b60e}
Computation in belief networks reduces to computing the sums of products:
\begin{itemize}
\item to compute \(ab + ac\) efficiently
\begin{itemize}
\item distribute to \(a(b + c)\)
\end{itemize}
\item to compute \(\sum_{Z_{1}} \prod_{i=1}^{n} P(X_{i} \mid \text{parents}(X_{i}))\) efficiently
\begin{itemize}
\item distribute out the factors that don't involve \(Z_{1}\)
\end{itemize}
\end{itemize}
\subsection{Algorithm}
\label{sec:org8e31e25}
To compute \(P(Z \mid Y_{1} = v_{1} \wedge \cdots \wedge Y_{j} = v_{j})\):
\begin{itemize}
\item construct a factor for each conditional probability
\item restrict the observed variables to their observed values
\item sum out each of the other variables according to some \textbf{elimination ordering} for
each \(Z_{i}\) in order starting from \(i = 1\)
\begin{itemize}
\item collect all factors that contain \(Z_{i}\)
\item multiple together and sum out \(Z_{i}\)
\item add resulting new factor back to the pol
\end{itemize}
\item multiply the remaining factors
\item normalize by dividing the resulting factor \(f(Z)\) by \(\sum_{Z} f(Z)\)
\end{itemize}
\subsection{Summing out a Variable}
\label{sec:org8804133}
To sum out a variable \(Z_{j}\) from a product \(f_{1}, \dots, f_{k}\) of factors:
\begin{itemize}
\item partition the factors into
\begin{itemize}
\item those that don't contain \(Z_{j}\), let these be \(f_{1}, \dots, f_{i}\)
\item those that contain \(Z_{j}\), let these be \(f_{i+1}, \dots, f_{k}\)
\end{itemize}
\end{itemize}

This gives
$$ \sum_{Z_{j}} f_{1} \times \cdots \times f_{k} = f_{1} \times \cdots \times f_{i} \times \left( \sum_{Z_{j}} f_{i+1} \times \cdots \times f_{k} \right) $$

Explicitly construct a representation of the rightmost factor and replace
the factors by the new factor.
\subsection{Elimination Ordering}
\label{sec:org74eafb8}
Complexity is \uline{linear} in the number of variables and \uline{exponential} in the size of the largest
factor.
Creating new factors blows this up, but this depends on the \textbf{elimination ordering}.

For polytrees, work outside in.
For general BNs, this can be hard.
Specifically, finding the optimal elimination ordering is NP-hard for general BNs.

In general, inference is NP-hard.
\subsubsection{Polytrees}
\label{sec:orga706720}
Eliminate singly connected nodes first.
Then, no factor is ever larger than original CPTs.

If the most connected nodes are eliminated first, a large factor is created with the singly
connected nodes.
\subsubsection{Relevance}
\label{sec:org9ea955b}
Certain variables have no impact.

Can restrict attention to relevant variables.
Given query \(Q\) and evidence \(\mathbf{E}\), complete approximation is:
\begin{itemize}
\item \(Q\) is relevant
\item if any node is relevant, its parents are relevant
\item if \(E \in \mathbf{E}\) is a descendent of a relevant variable, then \(E\) is relevant
\end{itemize}

\textbf{Irrelevant variable}: a node that is not an ancestor of a query or evidence variable

This will only remove irrelevant variables, but may not remove them all.
\end{document}
