% Created 2024-11-20 Wed 22:49
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{parskip,darkmode}
\usepackage{algpseudocode}
\enabledarkmode
\author{DESKTOP-H800RKQ}
\date{\today}
\title{Unsupervised Machine Learning}
\hypersetup{
 pdfauthor={DESKTOP-H800RKQ},
 pdftitle={Unsupervised Machine Learning},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.7.11)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Motivation: Missing/Incomplete Data}
\label{sec:org3d6e7e9}
Many real-world problems have \textbf{hidden (latent) variables} which have incomplete data
or values of some attributes missing.
This requires \textbf{unsupervised learning}.

To deal with missing data:
\begin{enumerate}
\item ignore hidden variables, instead drawing connections between other variables directly
\item ignore records with missing values (does not work with variables always missing)
\begin{enumerate}
\item cannot ignore missing data unless it is missing at random (but it is often correlated
with a variable of interest) \(\to\) must model why data is missing
\end{enumerate}
\item maximize likelihood directly: suppose \(Z\) is hidden and \(E\) is observable with values \(e\)
$$ h_{ML} = \text{arg} \max_{h} P(e \mid h) = \text{arg} \max_{h} \left[ \log \sum_{Z} \prod_{i=1}^{n} P(X_{i} \mid \text{parents}(X_{i}), h)_{E = e} \right] $$
\begin{enumerate}
\item the issue here is that the logarithm cannot be pushed into the sum to linearize
\end{enumerate}
\item if the missing values are known, \(h_{ML}\) would be easy
\begin{enumerate}
\item this can be done with \textbf{Expectation Maximization}: guess \(h_{ML}\) and iterate
\item \textbf{expectation}: based on \(h_{ML}\), compute expectation of missing values \(P(Z \mid h_{ML}, e)\)
\item \textbf{maximization}: based on expected missing values, compute new estimate of \(h_{ML}\)
\end{enumerate}
\item simple version (K-means algorithm)
\begin{enumerate}
\item \textbf{expectation}: based on \(h_{ML}\), compute most likely missing values
\(\text{arg} \max_{Z} P(Z \mid h_{ML}, e)\)
\item \textbf{maximization}: based on expected missing values, using complete data, compute new
estimate of \(h_{ML}\) using ML learning as before
\end{enumerate}
\end{enumerate}
\section{Expectation Maximization}
\label{sec:org62a3a18}
\textbf{k-means algorithm} can be used for clustering: dataset of observables with input
features \(X\) generated by one of a set of classes \(C\)

Inputs:
\begin{itemize}
\item training examples
\item number of classes \(k\)
\end{itemize}

Outputs:
\begin{itemize}
\item a representative value for each input feature for each class
\item an assignment of examples to classes
\end{itemize}

\uline{Algorithm}:
\begin{enumerate}
\item pick \(k\) means in \(X\), one per class \(C\)
\item do the following until means stop changing
\begin{enumerate}
\item assign examples to \(k\) classes (closest to current means)
\item re-estimate \(k\) means based on assignment
\end{enumerate}
\end{enumerate}
\subsection{E Step}
\label{sec:orgb1f780b}
Take the missing data, like the class, and fill in the data with probabilities from
maximum likelihood.
This should have data like the class and the probablity for that class, and let
this be \(A[X_{1}, \dots, X_{n}, C]\).
\subsection{M Step}
\label{sec:org69c1fd5}
Compute the statistics for each feature and class:
$$ M_{i}[X_{i}, C] = \sum_{X_{1}, \dots, X_{i-1},X_{i+1}, \dots, X_{n}} A[X_{1}, \dots, X_{n}, C] $$
$$ M[C] = \sum_{X_{i}} M_{i} [X_{i}, C] $$
Here, \(M[C]\) is an unnormalized marginal, so this should be normalized by:
$$ P(X_{i} \mid C) = M_{i}[X_{i}, C] / M[C] $$
$$ P(C) = M[C] / s $$
where \(s\) is the corresponding sum.
Pseudo-counts can also be added here when normalizing.
\subsection{EM Formal}
\label{sec:org4883ec3}
Approximate the maximum likelihood, by starting with a guess \(h_{0}\).
Then iteratively compute
$$ h_{i+1} = \text{arg} \max_{h} \sum_{Z} P(Z \mid h_{i}, e) \log P(e, Z \mid h) $$

Here, the \textbf{expectation} step is to compute \(P(Z \mid h_{i}, e)\) (fill in missing data).
Then, the \textbf{maximization} step is to find a new \(h\) that maximizes the above sum.

Note that
$$ \log P(e \mid h) \ge \sum_{Z} P(Z \mid e, h) \log P(e, Z \mid h) $$
Since EM finds a local maximum of the right side, this is a lower bound of the left side.
Further, the log inside the sum can linearize the product:
$$ h_{i+1} = \text{arg} \max_{h} \sum_{Z} P(Z \mid h_{i}, e) \sum_{j=1}^{n} \log P(X_{i} \mid \text{parents}(X_{i}), h)_{E = e} $$

EM monotonically improves the likelihood, so \(P(e \mid h_{i+1}) \ge P(e \mid h_{i})\).
\subsection{General Bayes Network EM}
\label{sec:org75a8052}
With complete data, use Bayes Net Maximum Likelihood:

With incomplete data, use Bayes Net Expectation Maximization.
With observed variables \(X\) and missing variables \(Z\),
start with some guess for \(\theta\).

In the \uline{E step}: compute weights for each data \(x_{i}\) and latent variables \(z_{j}\)
(using variable elimination)
$$ w_{ij} = P(z_{j} \mid \theta, x_{i}) $$

In the \uline{M step}: update parameters
$$ \theta_{V = j, \text{parents}(V) = v} = \frac{\sum_{i} w_{ij} \mid V = j \wedge \text{parents}(V) = v \text{ in } \{x_{i}, z_{j}\}}{\sum_{ij} w_{ij} \mid \text{parents}(V) = v \text{ in } \{x_{i}, z_{j}\}} $$
\section{Belief Network Structure Learning}
\label{sec:orgab45c25}
$$ P(\text{model} \mid \text{data}) = \frac{P(\text{data} \mid \text{model}) \times P(\text{model})}{P(\text{data})} $$
A model here is a belief network.
A bigger network can always fit data better.
\(P(\text{model})\) allows encoding of a preference for smaller networks.
Can search over network structure, looking for the most likely model.

Can do \textbf{independence tests} to determine which features should be the parents.
Just because features do not give information individually does not mean they will not give info
in combination.

The ideal method is to search over total orderings of variables.
\section{Autoencoders}
\label{sec:org0c1732d}
A representation learning algorithm that learns to map examples to low-dimensional representation.

An autoencoder has two main components:
\begin{enumerate}
\item \textbf{encoder} \(e(x)\): maps \(x\) to low-dimensional representation \(\hat{z}\)
\item \textbf{decoder} \(d(\hat{z})\): maps \(\hat{z}\) to its original representation \(x\)
\end{enumerate}

The autoencoder implements \(\hat{x} = d(e(x))\), so \(\hat{x}\) is the reconstruction of the original
input \(x\).
The encode and decoder learn such that \(\hat{z}\) contains as much info about \(x\) as is needed
to reconstruct it.

The goal is the minimize the sum of squares of differences between the input and prediction:
$$ E = \sum_{i} (x_{i} - d(e(x_{i})))^{2} $$

Deep neural network autoencoders are good for complex inputs,
where \(e\) and \(d\) are feedforward neural networks, joined in series.
The network is then trained with backpropagation.
\section{Generative Adversarial Networks}
\label{sec:org511f2d1}
A generative unsupervised learning algorithm that learns to generate unseen examples that
look like training examples.

GANs are a pair of neural networks:
\begin{enumerate}
\item \textbf{generator} \(g(z)\): given vector \(z\) in latent space, produce an example \(x\) drawn from a
distribution that approximates the true distribution of training examples, where \(z\)
is usually sampled from a Gaussian distribution
\item \textbf{discriminator} \(d(x)\): a classifier that predicts whether \(x\) is real (from training set)
or fake (made by \(g\))
\end{enumerate}

GANs are trained with a minimax error:
$$ E = \mathbb{E}_{x} [ \log( d(x) )] + \mathbb{E}_{z}[ \log(1 - d(g(z)))] $$
where the discriminator tries to maximize \(E\) and the generator tries to minimize \(E\).
Once they converge, \(g\) should be producing realistic examples and \(d\) should output
\(\frac{1}{2}\) to indicate maximal uncertainty.
\end{document}
