% Created 2024-11-21 Thu 00:45
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{parskip,darkmode}
\usepackage{upgreek}
\enabledarkmode
\author{DESKTOP-H800RKQ}
\date{\today}
\title{Planning With Uncertainty Decision Networks}
\hypersetup{
 pdfauthor={DESKTOP-H800RKQ},
 pdftitle={Planning With Uncertainty Decision Networks},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.7.11)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Preferences}
\label{sec:org1bb6cb5}
Let \(V(x)\) be the \textbf{utility} of the situation \(X\).
\textbf{Bayesian decision making} is
$$ \mathbb{E}(V(\text{decision})) = \sum_{\text{outcome}} P(\text{outcome} \mid \text{decision}) V(\text{outcome}) $$
Context can also be added to \(V\) so the utility is the value of the decision in the situation context.
$$ \mathbb{E}(V(\text{decision})) = \sum_{\text{outcome}} P(\text{outcome} \mid \text{decision, context}) V(\text{outcome}) $$

Actions result in outcomes and agents have \textbf{preferences} over outcomes.
A decision-theoretic \textbf{rational} agent will do the action that has the best outcome for them.
Agents have to act (even if doing nothing) and compare actions without knowing the
outcomes of actions.

If \(o_{1}\) and \(o_{2}\) are outcomes
\begin{itemize}
\item \uline{weak preference}: \(o_{1} \succeq o_{2}\) means \(o_{1}\) is at least as desirable as \(o_{2}\)
\item \uline{indifference}: \(o_{1} \sim o_{2}\) means \(o_{1} \succeq o_{2}\) and \(o_{2} \succeq o_{1}\)
\item \uline{strong preference}: \(o_{1} \succ o_{2}\) means \(o_{1} \succeq o_{2}\) and \(o_{2} \not\succeq o_{1}\)
\end{itemize}


\textbf{Lottery}: probability distribution over outcomes, denoted
$$ [p_{1} : o_{1}, \dots, p_{k} : o_{k}] $$
where \(o_{i}\) are outcomes and \(p_{i} > 0\) such that
$$ \sum_{i} p_{i} = 1 $$
\subsection{Properties of Preferences}
\label{sec:org92ef946}
\textbf{Completeness}: agents have to act so they must have preferences
$$ \forall o_{1} \forall o_{2} \; o_{1} \succeq o_{2} \text{ or } o_{2} \succeq o_{1} $$

\textbf{Transitivity}: if \(o_{1} \succeq o_{2}\) and \(o_{2} \succeq o_{3}\), then \(o_{1} \succeq o_{3}\)

\textbf{Monotonicity}: an agent prefers a larger chance of getting a better outcome than a smaller
chance, so if \(o_{1} \succ o_{2}\) and \(p > q\) then
$$ [p : o_{1}, 1 - p : o_{2}] \succ [q : o_{1}, 1 - q : o_{2}] $$

\textbf{Continuity}: suppose \(o_{1} \succ o_{2}\) and \(o_{2} \succ o_{3}\), there then exists a
\(p \in [0,1]\) such that
$$ o_{2} \sim [1 - p : o_{1}, p : o_{3}] $$

\textbf{Decomposibility}: an agent is indifferent between lotteries that have same probabilities and
outcomes

\textbf{Substitutability}: if \(o_{1} \sim o_{2}\) then the agent is indifferent between lotteries that
only differ by \(o_{1}\) and \(o_{2}\)

If preferences follow these properties, they can be measured by the function
$$ \text{utility} : \text{outcome} \to [0,1] $$
such that
\begin{itemize}
\item \(o_{1} \succeq o_{2}\) if and only if \(\text{utility}(o_{1}) \ge \text{utility}(o_{2})\)
\item utilities are linear with probabilities
$$ \text{utility}([p_{1} : o_{1}, \dots, p_{k} : o_{k}]) = \sum_{i=1}^{k} p_{i} \times \text{utility}(o_{i}) $$
\end{itemize}
\section{Rationality and Irrationality}
\label{sec:org0ec58b5}
Rational agents act to maximize expected utility.

Humans are not rational and weight value differently for gains and losses (prospect theory).
\section{Decisions}
\label{sec:orgd59f5bd}
Making a decision on what an agent should do depends on
\begin{itemize}
\item agent's \textbf{ability}: options available to it
\item agent's \textbf{beliefs}: how the world could be given the agent's knowledge, updating by sensing
world
\item agent's \textbf{preferences}: what the agent actually wants and tradeoffs present when
there are risks
\end{itemize}

Decision theory specifies how to trade off desirability and probabilities of the possible
outcomes for competing actions.

\textbf{Decision variables}: random variables that an agent gets to choose the values of, specifically
it can choose any value in the domain

\textbf{Expected utility} of a decision leads to outcomes \(\omega\) for utility function \(u\) is
$$ \mathcal{E}(u \mid D = d_{i}) = \sum P(\omega \mid D = d_{i})u(\omega) $$

\textbf{Optimal single decision}: decision \(D = d_{\max}\) whose expected utility is maximal:
$$ \mathcal{E}(u \mid D = d_{\max}) = \max_{d_{i} \in \text{dom}(D)} \mathcal{E}(u \mid D = d_{i}) $$
\section{Decision Networks}
\label{sec:org5c4e6df}
Graphical representation of a finite sequential decision problem.
Extension of belief networks to include decision variables and utility.

A decision network specifies
\begin{itemize}
\item what info is available when the agent has to act
\item which variables the utility depends on
\end{itemize}

A \uline{random variable} is drawn as an ellipse, with arcs into the node representing
probabilistic dependence.
A \uline{decision variable}  is drawn as a rectangle, with arcs into the node representing info
available when the decision is made.
A \uline{utility node} is drawn as a diamond, with arcs into the node representing variables that
the utility depends on.

Assume random variables \(X_{1}, \dots, X_{n}\), decision
variables \(D\), and utility dependent on \(X_{i_{1}}, \dots, X_{i_{k}}\) and \(D\):
$$ \mathcal{E}(u \mid D) = \sum_{X_{1}, \dots, X_{n}} \prod_{j=1}^{n} P(X_{j} \mid \text{parents}(X_{j})) \times u(X_{i_{1}}, \dots, X_{i_{k}}, D) $$
where \(\text{parents}(X_{j})\) may include \(D\).
To find the optimal decision:
\begin{itemize}
\item create a factor for each conditional probability and for the utility
\item multiply these together and sum out all random variables, creating a factor on \(D\)
that gives the expected utility for each \(D\)
\item choose the \(D\) with the maximum value in the factor
\end{itemize}
\section{Sequential Decisions}
\label{sec:orgffaa563}
An intelligent agent observes and acts iteratively, where subsequent
actions can depend on what is observed, which depends on previous actions.

Often an action is carried out to provide info for future actions.

\textbf{Sequential decision problem}: consists of a sequence of decision variables
\(D_{1}, \dots, D_{n}\) where each \(D_{i}\) has an info set of variables
\(\text{parents}(D_{i})\) whose value will be known when decision \(D_{i}\)
is made
\section{Policies}
\label{sec:org689708e}
Specifies what an agent should do under each circumstance.

A \textbf{policy} is a sequence \(\delta_{1}, \dots, \delta_{n}\) of decision functions
$$ \delta_{i} : \text{dom}(\text{parents}(D_{i})) \to \text{dom}(D_{i}) $$
which means that when the agent has observed \(O \in \text{dom}(\text{parents}(D_{i}))\)
it will do \(\delta_{i}(O)\).

A world \(\omega\) satisfies a policy \(\delta\), denoted \(\omega \models \delta\) if the
decisions of the policy are those the world assigns to the decision variables.

The \textbf{expected utility} of a policy \(\delta\) is
$$ \mathcal{E} ( u \mid \delta ) = \sum_{\omega \models \delta} u(\omega) \times P(\omega) $$
\textbf{Optimal policy} comes from maximizing expected utility.

To find an optimal policy:
\begin{enumerate}
\item create a factor for each conditional probability table and a factor for the utility
\item set remaining decision nodes to all decision nodes
\item multiply factors and sum out variables that are not parents of a remaining decision node
\item select and remove a decision variable \(D\) from the list of remaining decision nodes,
picking one that is in a factor with only itself and some of its parents (no children)
\item eliminate \(D\) by maximizing, which returns the optimal decision function for \(D\),
\(\text{arg} \max_{D} f\), and a new factor to use \(\max_{D} f\)
\item repeat steps 3 to 5 until there are no more remaining decision nodes
\item eliminate the remaining random variables by multiplying th factors, which is the
expected utility of the optimal policy
\item if any nodes were in evidence, divide by the \(P(\text{evidence})\)
\end{enumerate}
\end{document}
