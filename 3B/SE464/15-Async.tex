% Created 2024-12-10 Tue 20:03
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{parskip,darkmode}
\enabledarkmode
\author{Arnav Gupta}
\date{\today}
\title{Asynchronous Processing}
\hypersetup{
 pdfauthor={Arnav Gupta},
 pdftitle={Asynchronous Processing},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.7.11)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Motivation}
\label{sec:org963d81d}
Responses acknowledge request received and can contain data or info about failure
that client needs to react to.

Synchronous APIs have \textbf{blocking} request where the client waits for the server to
finish processing, hence the two are synchronized.

If request very long, client may not care to wait until message delivery is
verified.
\section{Asynchronous Processing}
\label{sec:orgc84ea70}
Acknowledge request received first, then finish work later.

Allows client to quickly move on, but client does not learn whether request
succeeded.

In synchronous, delivery is acknowledged. In asynchronous, attempt is
acknowledged.

Sometimes client will proceed immediately after send, but later want to now
whether request succeeded or to get response data.
\subsection{Request Record}
\label{sec:org1aa4098}
Server can store request record in DB and return unique ID.
When done, server updates request record in DB.

Client can later check on results using request ID.
\subsection{Callback to Client}
\label{sec:org81fdb9f}
Client provides a callback function (webhook) where it expects to receive
response.
Only works if client can listen for responses (always running, not NATed, etc).
\subsection{Side Channel for Feedback}
\label{sec:orgb45dbdc}
Let clients send requests when failure is rare.

If failure occurs, report error to another system (like log).
\section{Message Queues}
\label{sec:org5aebe92}
Provide asynchronicity and decoupling.

If client doesn't care about response status, it can just put request on a queue.

Adding a message to a queue very fast since just data copy, without any parsing or
business logic.

Prevents slowdown of upstream service due to downstream congestion, so system can
handle short bursts of traffic beyond system capacity.

Putting message on queue similar to making API request, so content of message
defines request.

Rules for formatting are a contract like an API.

\textbf{Producer}: pushes/publishes/produces messages

\textbf{Consumer}: pulls/pops/consumers/subscribes-to messages

Message queue can be partitioned into several virtual queues by assigning
\textbf{topic} to each message, where consumers subscribe to some subset of all
topics.
\subsection{Tradeoffs}
\label{sec:orga07cd0c}
Tightly coupled (synchronous) services simpler to design/build.

Loosely coupled (asynchronous) services faster but:
\begin{itemize}
\item failures must be unimportant and ignored
\item errors might be stored in DB and somehow checked later
\begin{itemize}
\item can be difficult to sensibly react to error at later time
\end{itemize}
\item errors can lead to alert to user later

Decoupling helps scaling, since many producers and consumers can
connect to the queue.

Basic queues run on 1 machine and distributed queues run on clusters.
Like load balancer, queue allows work to be distributed.

Producer and consumer can be scaled separately as needed.

Queue smooths demand peaks by deferring work.
\end{itemize}
\subsection{Types of Queues}
\label{sec:org12f82be}
\subsubsection{Passive Queue}
\label{sec:orga09627b}
Accepts and stores messages until they are requested (like a specialized
DB).

Consumers must periodically request messages (poll).

Producer pushes and consumer pulls.
\subsubsection{Active Queue}
\label{sec:orgb8bb188}
Queue knows where to send messages and actively pushes messages out
to subscribers.

Subscribers must listen for messages.

Producer pushes and queue pushes to consumer.
\subsection{Architectural Levels}
\label{sec:org8cd56f4}
\textbf{In-app Queue}: app defines its own queue to store work that it will
do later, perhaps in a different thread
\begin{itemize}
\item simple and no separate app to deploy
\item usually not stored on disk, so app crash/reboot may drop queued
messages
\end{itemize}

\textbf{Separate Queuing App}: process that listens for pushes/fetches on a
network connection
\begin{itemize}
\item can often run as a process on the same VM as the application pushing
to it (communication stays local)
\item can reside on existing app VM and can write queued messages to a file
\item scalability limited to 1 machine so machine/disk failure drops
messages
\end{itemize}

\textbf{Distributed Message Queue}: cluster of nodes that together implement a
robust, scalable queue, allowing all work to go to the big queue
\begin{itemize}
\item massively scalable as messages are replicated on many nodes
\item provides a single point of coordination for many producers/consumers
\item complexity is an issue as well as consistency side effects (must choose
delivery guarantee to be at least once or at most once, never exactly
once)
\end{itemize}
\subsection{Back Pressure}
\label{sec:org5365318}
If queue fills up, it should be possible for queue to give error response to
producer adding message.

This will stall service, so DevOps/operations staff should monitor queue sizes
to anticipate such problems.
\subsubsection{Ordering}
\label{sec:orgf218434}
Distributed queue cannot guarantee strict FIFO ordering of messages.

If messages must be ordered, send multiple as a single big message instead.
\subsection{Implementation}
\label{sec:org52fdbc1}
Message queues should be connected to backend rather than frontend, since they
are not designed to accept public requests or connections from 1000s of clients.
\end{document}
